{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mc-Call Job Search Model with Separation\n",
    "\n",
    "We consider here a very simple job-search model, with separation. Our goal consists in solving this model using a policy iteration algorithm. Some background on the McCall and numerical ideas are discussed on the [QuantEcon](`https://julia.quantecon.org/mccall_model.html`) website.\n",
    "\n",
    "There is a single worker who can be either employed (\"e\") or unemployed (\"u\") in any period.\n",
    "\n",
    "When unemployed, the jobless worker receives unemployment benefits $c_t=\\alpha>0$ in every period as long as he stays unemployed. He also receives a salary offer $w_t$ which is drawn from a discrete i.i.d. distribution and takes values $w_1, ..., w_K$ with probabilities $p_1, ... p_K$ respectively.\n",
    "\n",
    "When an unemployed worker accepts an offer in period $t$, he gets the salary $w_t$ and becomes employed. He then keeps his salary $w_t$ as long as he stays employed (for $s\\geq t$, $c_s=w_t$ if $t$ is the date at which worker got the current job); in each period he has a probability $\\lambda$ of becoming unemployed in the next period and remains employed otherwise.\n",
    "\n",
    "When a worker receives a given amount $x$ his perceived utility is $U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}$ with $\\gamma>1.0$. A worker discounts the future at a rate $\\beta \\in [0,1[$. As a result, in any period $t_0$ workers seek to maximize $\\sum_{t\\geq t_0}^{\\infty} U(c_t)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define a parameter type `Parameter`, with fields $\\alpha$, $\\beta$, $\\gamma$, $K$, $\\sigma$, $\\lambda$. Create a parameter variable $\\omega$ with $\\alpha=0.5$, $\\beta=0.96$, $\\gamma=4$, $K=10$, $\\sigma=0.6$, $\\lambda=0.015$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is the uniform distribution, whose mean is 1 and standard deviation is $\\sigma$? Write a function `discrete_uniform(σ::Float64, K::Int64)::Tuple{Vector{Float64} Vector{Float64}}` to discretize it, using $K$ points. The function should return two vectors `w` and `p` of floats of the same size `K`. Check the results satisfy the right conditions (uniformity, standard deviation).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The optimal decision of a worker is characterized by two value functions: $V^E(w)$ is the value of being employed at wage $w$ and $V^U(w)$ the value of being unemployed, while receiving job offer $w_t$. In Julia, both $V^U$ and $V^E$ will be represented by arrays `V_U` and `V_E` of size `K`.\n",
    "\n",
    "A policy `g(w)` is a binary choice in the unemployed state: accept or reject an offer $w_t$. It will then naturally be represented by a boolean array (type `zeros(Bool, K)` to initialize one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Given a policy $g$, write down the recursive equations which defines the corresponding value functions $V^{U,g}(w)$ and $V^{E,g}(w)$.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function which takes a guess $V^{U,g}(w)$ and $V^{E,g}(w)$ and a policy function $g$ as arguments (and other model parameters) and updates the values, according to the updating equations. This function could have signature `value_update(V_U::Vector{Float64}, V_E::Vector{Float64}, g::Vector{Bool}, ω::Parameter, w::Vector{Float64}, p::Vector{Float64})::Vector{Float64}` where the returned vector has the same size as the supplied ones.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Write a function `eval_policy(g::Vector{Bool}, ω::Parameter, w::Vector{Float64}, p::Vector{Float64}, η::Float64)::Tuple{Vector{Float64}, Vector{Float64}}` which iterates on `value_update` find the values that satisfy the evaluation equations for policy `g`.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Write a function `improve_policy(g::Vector{Bool}, V_U::Vector{Bool}, V_E::Vector{Bool}, ω::Parameter, w::Vector{Float64}, p::Vector{Float64}, η::Float64)::Vector{Float64}` which returns the improved policy given guesses for the value function(s) at for $t+1$.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implement the policy function algorithm. Print the successive approximation errors and comment on the convergence speed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neoclassical growth model (2)\n",
    "\n",
    "We consider here, another deterministic version of the neoclassical growth model, but propose a slightly different solution method.\n",
    "\n",
    "A representative agent uses capital $k_t$ to produce $y_t$ using the following production function:\n",
    "\n",
    "$$y_t = k_t^{\\alpha}$$\n",
    "\n",
    "He chooses to consume an amount $c_t \\in ]0, y_t]$ and invests what remains:\n",
    "\n",
    "$$i_t = y_t - c_t$$.\n",
    "\n",
    "He accumulates capital $k_t$ according to:\n",
    "\n",
    "$$k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}$$\n",
    "\n",
    "where $\\delta$ is the depreciation rate and $i_t$ is the amount invested.\n",
    "\n",
    "The goal of the representative agent is to maximize:\n",
    "\n",
    "$$\\sum_{t\\geq 0} \\beta^t U(c_t)$$\n",
    "\n",
    "where $U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}$ and $\\beta<1$ is the discount factor.\n",
    "\n",
    "Since the problem is time homogenous, the value function depends on available capital only and satisfies the following Bellman equation:\n",
    "\n",
    "$$V\\left(\\underbrace{k}_{k_t}\\right) = \\max_{c\\in[0,1[} U(c) + \\beta V\\left(\\underbrace{(1-\\delta)k + \\underbrace{(k^{\\alpha}-c)}_{y_{t+1}}}_{k_{t+1}}\\right)$$\n",
    "\n",
    "Our goal is to obtain a smooth approximation of $k$ and $V$ by using interpolations techniques.\n",
    "\n",
    "For this model, using the dynamic first-order conditions, one can show the deterministic steady-state of the model satisfies $1=\\beta \\left( (1-\\delta) + \\alpha k^{\\alpha -1} \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a suitable Parameter type to hold the parameters. Write a function `steady_state(p::Parameter)` to compute the steady-state capital `kbar` and the corresponding steady-state consumption `cbar`__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set $N=10$ and define a reasonable grid `kgrid=range(kmin, kmax; length=N)` to approximate capital $k$.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The unknown value function is represented as a `N` elements arrays. Define `Vi(k,p)=U(δ * k^α)/(1-𝛽)` and compute the initial guess `V0 = [Vi(k,p) for k in kgrid]`. Define a finer grid `ktest=range(kmin, kmax;length=1000)` and find the values of `Vi` on it by  using `Interpolations.jl` library to interpolate `V0` between the points of `kgrid`.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Compute a Bellman improvement function `bellman(V0::Vector{Float64}, p::Parameter, kgrid)::Tuple{Vector{Float64}, Vector{Float64}}` which does the following steps:__\n",
    "\n",
    "- take an initial guess `V0` for the value function\n",
    "\n",
    "- at each grid point from kvec, optimize nonlinearly, the function $c \\rightarrow U(c) + \\beta V\\left((1-\\delta)k + (k^{\\alpha}-c)\\right)$ for each capital level in the grid `kvec`. In this expression the function `V()` interpolates `V0` defined on `kvec` on any point `k` so that the resulting function is continuous. \n",
    "\n",
    "- return the updated value and investment rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Write a value interation function `vfi(N, p)` which solves the model defined by parameter `p` using the value function algorithm. The function should return the value function and the policy rule.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the solution. Comment.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus 1: plot a graph showing the convergence back to the steady-state__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus 2: implement the policy iteration algorithm by adding an evaluation step in the `vfi` function.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
